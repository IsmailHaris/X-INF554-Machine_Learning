{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(q_value, beta=1.0):\n",
    "    assert beta >= 0.0\n",
    "    q_tilde = q_value - np.max(q_value)\n",
    "    factors = np.exp(beta * q_tilde)\n",
    "    return factors / np.sum(factors)\n",
    "\n",
    "def select_a_with_softmax(curr_s, q_value, beta=1.0):\n",
    "    prob_a = softmax(q_value[curr_s, :], beta=beta)\n",
    "    cumsum_a = np.cumsum(prob_a)\n",
    "    return np.where(np.random.rand() < cumsum_a)[0][0]\n",
    "\n",
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(arguments):\n",
    "\n",
    "    args = parse_arguements(arguments)\n",
    "\n",
    "    # General parameters\n",
    "    env_type         = args.environment\n",
    "    algorithm_type   = args.algorithm\n",
    "    policy_type      = args.policy\n",
    "\n",
    "    # Meta parameters for the reiforcement learning agent\n",
    "    alpha            = args.alpha\n",
    "    beta = init_beta = args.beta\n",
    "    beta_inc         = args.betainc\n",
    "    gamma            = args.gamma\n",
    "    epsilon          = args.epsilon\n",
    "    epsilon_decay    = args.epsilondecay\n",
    "    n_episode        = args.nepisode\n",
    "    max_step         = args.maxstep\n",
    "    \n",
    "    # Selection of the problem & constraints imposed by the environment\n",
    "    env = gym.envs.make(env_type)\n",
    "    n_a = env.action_space.n\n",
    "    n_s = env.observation_space.n\n",
    "    \n",
    "    # Initialization and configuration\n",
    "    q_table = np.zeros([n_s, n_a]) # Initialization of a Q-value table\n",
    "    history = [] # Initialization of a list for storing simulation history\n",
    "    env.reset()\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "    result_dir = 'results-{0}-{1}-{2}'.format(env_type, algorithm_type, policy_type)\n",
    "    \n",
    "    # Print main params\n",
    "    print(\"n_episode      : {}\".format(n_episode))\n",
    "    print(\"algorithm_type : {}\".format(algorithm_type))\n",
    "    print(\"policy_type    : {}\".format(policy_type))\n",
    "\n",
    "    for i_episode in range(n_episode):\n",
    "        score = 0 # Reset a cumulative reward for this episode\n",
    "        observation = env.reset() # Start a new episode and sample the initial state\n",
    "\n",
    "        # Select the first action in this episode\n",
    "        if policy_type == 'softmax':\n",
    "            action = select_a_with_softmax(observation, q_table, beta=beta)\n",
    "        elif policy_type == 'epsilon_greedy':\n",
    "            action = select_a_with_epsilon_greedy(observation, q_table, epsilon=epsilon)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "        for i_step in range(max_step):            \n",
    "            next_observation, reward, done, info = env.step(action) # Get a result of your action from the environment\n",
    "\n",
    "            # Modification of reward (not sure if it's OK to change reward setting by hand...)\n",
    "            if done & (reward == 0):\n",
    "                # Punishment for falling into a hall\n",
    "                reward = 0.0\n",
    "            elif not done:\n",
    "                # Cost per step\n",
    "                reward = -0.001\n",
    "\n",
    "            # Update a cummulative reward\n",
    "            score = reward + gamma * score\n",
    "\n",
    "            # Select an action\n",
    "            if policy_type == 'softmax':\n",
    "                next_a = select_a_with_softmax(next_observation, q_table, beta=beta)\n",
    "            elif policy_type == 'epsilon_greedy':\n",
    "                next_a = select_a_with_epsilon_greedy(next_observation, q_table, epsilon=epsilon)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid policy_type: {}\".format(policy_type))            \n",
    "\n",
    "            # Calculation of TD error\n",
    "            if algorithm_type == 'sarsa':\n",
    "                delta = reward + gamma * q_table[next_observation, next_a] - q_table[observation, action]\n",
    "            elif algorithm_type == 'q_learning':\n",
    "                delta = reward + gamma * np.max(q_table[next_observation, :]) - q_table[observation, action]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid algorithm_type: {}\".format(algorithm_type))\n",
    "\n",
    "            # Update a Q value table\n",
    "            q_table[observation, action] += alpha * delta\n",
    "\n",
    "            observation = next_observation\n",
    "            action = next_a\n",
    "\n",
    "            if done:\n",
    "                if policy_type == 'softmax':\n",
    "                    print(\"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tBeta: {4:.3f}\".format(i_episode, i_step, score, reward, beta))\n",
    "                    history.append([i_episode, i_step, score, reward, beta])\n",
    "                elif policy_type == 'epsilon_greedy':                \n",
    "                    print(\"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tEpsilon: {4:.3f}\".format(i_episode, i_step, score, reward, epsilon))\n",
    "                    history.append([i_episode, i_step, score, reward, epsilon])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "                break\n",
    "\n",
    "        if policy_type == 'epsilon_greedy':\n",
    "            # epsilon is decayed expolentially\n",
    "            epsilon = epsilon * epsilon_decay\n",
    "        elif policy_type == 'softmax':\n",
    "            # beta is increased linearly\n",
    "            beta = init_beta + i_episode * beta_inc\n",
    "\n",
    "    history = np.array(history)\n",
    "\n",
    "    window_size = 100\n",
    "    def running_average(x, window_size, mode='valid'):\n",
    "        return np.convolve(x, np.ones(window_size)/window_size, mode=mode)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=[12, 8])\n",
    "    # Number of steps\n",
    "    ax[0, 0].plot(history[:, 0], history[:, 1], '.') \n",
    "    ax[0, 0].set_xlabel('Episode')\n",
    "    ax[0, 0].set_ylabel('Number of steps')\n",
    "    ax[0, 0].plot(history[window_size-1:, 0], running_average(history[:, 1], window_size))\n",
    "    # Cumulative reward\n",
    "    ax[0, 1].plot(history[:, 0], history[:, 2], '.') \n",
    "    ax[0, 1].set_xlabel('Episode')\n",
    "    ax[0, 1].set_ylabel('Cumulative rewards')\n",
    "    ax[0, 1].plot(history[window_size-1:, 0], running_average(history[:, 2], window_size))\n",
    "    # Terminal reward\n",
    "    ax[1, 0].plot(history[:, 0], history[:, 3], '.') \n",
    "    ax[1, 0].set_xlabel('Episode')\n",
    "    ax[1, 0].set_ylabel('Terminal rewards')\n",
    "    ax[1, 0].plot(history[window_size-1:, 0], running_average(history[:, 3], window_size))\n",
    "    # Epsilon/Beta\n",
    "    ax[1, 1].plot(history[:, 0], history[:, 4], '.') \n",
    "    ax[1, 1].set_xlabel('Episode')\n",
    "    if policy_type == 'softmax':\n",
    "        ax[1, 1].set_ylabel('Beta')\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        ax[1, 1].set_ylabel('Epsilon')\n",
    "    fig.savefig('./'+result_dir+'.png')\n",
    "\n",
    "    print(\"Q value table:\")\n",
    "    print(q_table)\n",
    "\n",
    "    if policy_type == 'softmax':\n",
    "        print(\"Action selection probability:\")\n",
    "        print(np.array([softmax(q, beta=beta) for q in q_table]))\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        print(\"Greedy action\")\n",
    "        greedy_action = np.zeros([n_s, n_a])\n",
    "        greedy_action[np.arange(n_s), np.argmax(q_table, axis=1)] = 1\n",
    "        #print np.array([zero_vec[np.argmax(q)] = 1 for q in q_table])\n",
    "        print(greedy_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
