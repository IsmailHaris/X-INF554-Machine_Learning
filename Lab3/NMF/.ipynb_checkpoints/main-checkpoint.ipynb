{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       Topic 1    Weight     Topic 2    Weight     Topic 3    Weight     Topic 4    Weight     Topic 5    Weight\n",
      "0     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332\n",
      "1     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332\n",
      "2        able  0.000332        able  0.000332        able  0.000332        able  0.000332        able  0.000332\n",
      "3      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332\n",
      "4    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332\n",
      "5      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332\n",
      "6     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332\n",
      "7  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332\n",
      "8      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332\n",
      "9       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332 \n",
      "\n",
      "\n",
      "       Topic 6    Weight     Topic 7    Weight     Topic 8    Weight     Topic 9    Weight    Topic 10    Weight\n",
      "0     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332\n",
      "1     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332\n",
      "2        able  0.000332        able  0.000332        able  0.000332        able  0.000332        able  0.000332\n",
      "3      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332\n",
      "4    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332\n",
      "5      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332\n",
      "6     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332\n",
      "7  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332\n",
      "8      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332\n",
      "9       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332 \n",
      "\n",
      "\n",
      "      Topic 11    Weight    Topic 12    Weight    Topic 13    Weight    Topic 14    Weight    Topic 15    Weight\n",
      "0     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332\n",
      "1     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332\n",
      "2        able  0.000332        able  0.000332        able  0.000332        able  0.000332        able  0.000332\n",
      "3      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332\n",
      "4    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332\n",
      "5      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332\n",
      "6     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332\n",
      "7  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332\n",
      "8      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332\n",
      "9       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332 \n",
      "\n",
      "\n",
      "      Topic 16    Weight    Topic 17    Weight    Topic 18    Weight    Topic 19    Weight    Topic 20    Weight\n",
      "0     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332\n",
      "1     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332\n",
      "2        able  0.000332        able  0.000332        able  0.000332        able  0.000332        able  0.000332\n",
      "3      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332\n",
      "4    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332\n",
      "5      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332\n",
      "6     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332\n",
      "7  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332\n",
      "8      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332\n",
      "9       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332 \n",
      "\n",
      "\n",
      "      Topic 21    Weight    Topic 22    Weight    Topic 23    Weight    Topic 24    Weight    Topic 25    Weight\n",
      "0     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332     abandon  0.000332\n",
      "1     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332     ability  0.000332\n",
      "2        able  0.000332        able  0.000332        able  0.000332        able  0.000332        able  0.000332\n",
      "3      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332      aboard  0.000332\n",
      "4    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332    abortion  0.000332\n",
      "5      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332      abroad  0.000332\n",
      "6     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332     absence  0.000332\n",
      "7  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332  absolutely  0.000332\n",
      "8      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332      absorb  0.000332\n",
      "9       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332       abuse  0.000332 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nmf import *\n",
    "import matplotlib.pyplot as plt\n",
    "with open('data/nyt_data.txt') as f:\n",
    "    documents = f.readlines()\n",
    "documents = [x.strip().strip('\\n').strip(\"'\") for x in documents]\n",
    "\n",
    "# contains vocabs with rows as index\n",
    "with open('data/nyt_vocab.dat') as f:\n",
    "    vocabs = f.readlines()\n",
    "vocabs = [x.strip().strip('\\n').strip(\"'\") for x in vocabs]\n",
    "\n",
    "'''create matrix X'''\n",
    "numDoc = 1000\n",
    "numWord = 3012\n",
    "X = np.zeros([numWord,numDoc],dtype=float)\n",
    "\n",
    "for col in range(len(documents[:1000])):\n",
    "    for row in documents[col].split(','):\n",
    "        X[int(row.split(':')[0])-1,col] = float(int(row.split(':')[1]))\n",
    "\n",
    "X=X+np.ones((numWord,numDoc))*0.00000001\n",
    "\n",
    "rank = 25\n",
    "W,H,d_iter=nmf_factor(X,rank)\n",
    "\n",
    "fig= plt.figure(figsize = (15,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(range(100),d_iter[:100])\n",
    "plt.title('Plot of euclidean norm objective in 100 iterations')\n",
    "plt.ylabel('$||X-WH||$')\n",
    "plt.xlabel('iteration $t$')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### b. Ten words with the largest weight.\n",
    "\n",
    "'''normalize each column to sum to zero'''\n",
    "W_normed = W / np.sum(W,axis=0)\n",
    "\n",
    "\n",
    "'''for each column of W, list the 10 words having the largest weight and show the weight'''\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "vList = []\n",
    "\n",
    "for topic in range(rank):\n",
    "    v = pd.DataFrame(vocabs)\n",
    "    v[1] = W_normed[:,topic].round(6)\n",
    "    v = v.sort_values([1, 0], ascending=[0,1]).rename(index=int, columns={0: \"Topic {}\".format(topic+1), 1: \"Weight\"}).head(10)\n",
    "    v = v.reset_index(drop=True)\n",
    "    vList.append(v)\n",
    "\n",
    "for num in [5,10,15,20,25]:\n",
    "    print('\\n',(pd.concat(vList[num-5:num], axis=1)),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
